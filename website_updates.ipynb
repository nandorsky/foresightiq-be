{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f55fa62-aa04-4cbc-96fd-f56e5b7e061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "import html2text\n",
    "from bs4 import BeautifulSoup\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from supabase import create_client, Client\n",
    "import random\n",
    "import requests\n",
    "import difflib\n",
    "import numpy as np\n",
    "import re\n",
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "# Supabase API Key\n",
    "SUPABASE_URL = os.environ[\"SUPABASE_URL\"]\n",
    "SUPABASE_KEY = os.environ[\"SUPABASE_KEY\"]\n",
    "SERVICE_ROLE_KEY = os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"]\n",
    "supabase: Client = create_client(SUPABASE_URL, SERVICE_ROLE_KEY)\n",
    "\n",
    "# Firecrawl API\n",
    "firecrawl_api_key = \"fc-6a9dd63b67a64375889bb608bee9664a\"\n",
    "\n",
    "# User agents for scrapping\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.67\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Mobile Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Linux; Android 13; SAMSUNG SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/21.0 Chrome/115.0.0.0 Mobile Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.5790.171 Safari/537.36 OPR/100.0.4815.76\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13.4; rv:109.0) Gecko/20100101 Firefox/109.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "482af851-5182-4b42-b103-70540dcc730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully fetched `change_log` (56097 items).\n",
      "‚úÖ Successfully fetched `competitors` (60 items).\n",
      "‚úÖ Successfully fetched `page_indexing` (8403 items).\n",
      "‚úÖ Successfully fetched `companies` (12 items).\n"
     ]
    }
   ],
   "source": [
    "def fetch_data(table_name, batch_size=1000, join_fields=None, flatten_joins=False):\n",
    "    try:\n",
    "        all_data = []\n",
    "        start = 0\n",
    "\n",
    "        # Build select clause\n",
    "        select_parts = [\"*\"]\n",
    "        if join_fields:\n",
    "            for table, fields in join_fields.items():\n",
    "                fields_str = \",\".join(fields)\n",
    "                select_parts.append(f\"{table}({fields_str})\")\n",
    "        select_clause = \", \".join(select_parts)\n",
    "\n",
    "        while True:\n",
    "            response = (\n",
    "                supabase\n",
    "                .table(table_name)\n",
    "                .select(select_clause)\n",
    "                .range(start, start + batch_size - 1)\n",
    "                .execute()\n",
    "            )\n",
    "\n",
    "            if response.data:\n",
    "                all_data.extend(response.data)\n",
    "                start += batch_size\n",
    "                if len(response.data) < batch_size:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not all_data:\n",
    "            print(f\"‚ö†Ô∏è `{table_name}` is empty.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Optional flattening\n",
    "        if flatten_joins and join_fields:\n",
    "            for join_table in join_fields.keys():\n",
    "                if join_table in df.columns:\n",
    "                    nested_df = pd.json_normalize(df[join_table])\n",
    "                    nested_df.columns = [f\"{join_table}_{col}\" for col in nested_df.columns]\n",
    "                    df = df.drop(columns=[join_table]).join(nested_df)\n",
    "\n",
    "        print(f\"‚úÖ Successfully fetched `{table_name}` ({len(df)} items).\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching data from '{table_name}': {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch tables\n",
    "change_log = fetch_data(\"change_log\")\n",
    "competitors = fetch_data(\"competitors\")\n",
    "\n",
    "# Fetch page_indexing WITH related competitor data\n",
    "page_indexing = fetch_data(\n",
    "    \"page_indexing\",\n",
    "    join_fields={\"competitors\": [\"company_id\"]},\n",
    "    flatten_joins=True\n",
    ")\n",
    "companies = fetch_data(\"companies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8081459-93a1-41bd-bd3f-c8d607be615b",
   "metadata": {},
   "source": [
    "## Filter down to active or trial companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed11d9d-dccc-4d8e-9a55-517a356d3bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 8403\n",
      "Rows after:  3588\n"
     ]
    }
   ],
   "source": [
    "active_companies = companies[\n",
    "    companies[\"status\"].isin([\"active\", \"trial\"])\n",
    "][[\"id\"]]\n",
    "\n",
    "filtered_page_indexing = page_indexing[\n",
    "    page_indexing[\"competitors_company_id\"].isin(active_companies[\"id\"])\n",
    "]\n",
    "\n",
    "print(f\"Rows before: {len(page_indexing)}\")\n",
    "print(f\"Rows after:  {len(filtered_page_indexing)}\")\n",
    "page_indexing = filtered_page_indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54049764-7545-4e25-a760-5ffb354da090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23,  7, 66, 71])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_indexing[\"competitors_company_id\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28b0a7-cb2a-4e2b-940a-96abe6790600",
   "metadata": {},
   "source": [
    "## Scrape pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c17889-ee86-4ffb-b29b-ffba343eb3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è Requesting 8276 | https://docs.nabla.com/guides/intro\n",
      "‚û°Ô∏è Requesting 8285 | https://docs.nabla.com/guides/intro/\n",
      "‚û°Ô∏è Requesting 2119 | https://www.happynest.com/locations/new-jersey\n",
      "‚û°Ô∏è Requesting 8440 | https://docs.nabla.com/server/oauth-generate-server-access-token\n",
      "‚û°Ô∏è Requesting 8443 | https://docs.nabla.com/guides/api-versioning/changelog-and-upgrades\n",
      "‚û°Ô∏è Requesting 8446 | https://docs.nabla.com/user/get-generate-note-async\n",
      "‚û°Ô∏è Requesting 8449 | https://docs.nabla.com/next/user/update-user-dot-phrase\n",
      "‚û°Ô∏è Requesting 8452 | https://docs.nabla.com/user/update-user-custom-dictionary-expression\n",
      "‚û°Ô∏è Requesting 8455 | https://docs.nabla.com/server/core-server-api\n",
      "‚û°Ô∏è Requesting 8458 | https://docs.nabla.com/user/delete-user-dot-phrase\n",
      "‚úÖ Scrapy success: https://docs.nabla.com/guides/intro\n",
      "‚ö†Ô∏è Scrapy failed: https://docs.nabla.com/guides/intro/ ‚Üí trying requests\n",
      "‚ö†Ô∏è Requests failed (404)\n",
      "üî• Trying Firecrawl: https://docs.nabla.com/guides/intro/\n",
      "‚ùå Firecrawl exception for https://docs.nabla.com/guides/intro/: 'Firecrawl' object has no attribute 'scrape_url'\n",
      "‚ùå Firecrawl failed: https://docs.nabla.com/guides/intro/\n",
      "‚úÖ Scrapy success: https://docs.nabla.com/next/user/update-user-dot-phrase\n",
      "‚úÖ Scrapy success: https://www.happynest.com/locations/new-jersey\n",
      "‚úÖ Scrapy success: https://docs.nabla.com/server/core-server-api\n",
      "‚úÖ Scrapy success: https://docs.nabla.com/user/delete-user-dot-phrase\n",
      "‚úÖ Scrapy success: https://docs.nabla.com/guides/api-versioning/changelog-and-upgrades\n",
      "‚úÖ Scrapy success: https://docs.nabla.com/user/update-user-custom-dictionary-expression\n",
      "‚úÖ Scrapy success: https://docs.nabla.com/server/oauth-generate-server-access-token\n",
      "‚úÖ Scrapy success: https://docs.nabla.com/user/get-generate-note-async\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_url</th>\n",
       "      <th>html_content</th>\n",
       "      <th>markdown_content</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8276</td>\n",
       "      <td>https://docs.nabla.com/guides/intro</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\" cla...</td>\n",
       "      <td>None</td>\n",
       "      <td>scrapy_success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8285</td>\n",
       "      <td>https://docs.nabla.com/guides/intro/</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>firecrawl_failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8449</td>\n",
       "      <td>https://docs.nabla.com/next/user/update-user-d...</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\" cla...</td>\n",
       "      <td>None</td>\n",
       "      <td>scrapy_success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2119</td>\n",
       "      <td>https://www.happynest.com/locations/new-jersey</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;\\n    &lt;me...</td>\n",
       "      <td>None</td>\n",
       "      <td>scrapy_success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8455</td>\n",
       "      <td>https://docs.nabla.com/server/core-server-api</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\" cla...</td>\n",
       "      <td>None</td>\n",
       "      <td>scrapy_success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8458</td>\n",
       "      <td>https://docs.nabla.com/user/delete-user-dot-ph...</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\" cla...</td>\n",
       "      <td>None</td>\n",
       "      <td>scrapy_success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8443</td>\n",
       "      <td>https://docs.nabla.com/guides/api-versioning/c...</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\" cla...</td>\n",
       "      <td>None</td>\n",
       "      <td>scrapy_success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8452</td>\n",
       "      <td>https://docs.nabla.com/user/update-user-custom...</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\" cla...</td>\n",
       "      <td>None</td>\n",
       "      <td>scrapy_success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8440</td>\n",
       "      <td>https://docs.nabla.com/server/oauth-generate-s...</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\" cla...</td>\n",
       "      <td>None</td>\n",
       "      <td>scrapy_success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8446</td>\n",
       "      <td>https://docs.nabla.com/user/get-generate-note-...</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\" dir=\"ltr\" cla...</td>\n",
       "      <td>None</td>\n",
       "      <td>scrapy_success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id                                           page_url  \\\n",
       "0     8276                https://docs.nabla.com/guides/intro   \n",
       "1     8285               https://docs.nabla.com/guides/intro/   \n",
       "2     8449  https://docs.nabla.com/next/user/update-user-d...   \n",
       "3     2119     https://www.happynest.com/locations/new-jersey   \n",
       "4     8455      https://docs.nabla.com/server/core-server-api   \n",
       "5     8458  https://docs.nabla.com/user/delete-user-dot-ph...   \n",
       "6     8443  https://docs.nabla.com/guides/api-versioning/c...   \n",
       "7     8452  https://docs.nabla.com/user/update-user-custom...   \n",
       "8     8440  https://docs.nabla.com/server/oauth-generate-s...   \n",
       "9     8446  https://docs.nabla.com/user/get-generate-note-...   \n",
       "\n",
       "                                        html_content markdown_content  \\\n",
       "0  <!doctype html>\\n<html lang=\"en\" dir=\"ltr\" cla...             None   \n",
       "1                                               None             None   \n",
       "2  <!doctype html>\\n<html lang=\"en\" dir=\"ltr\" cla...             None   \n",
       "3  <!doctype html><html lang=\"en\"><head>\\n    <me...             None   \n",
       "4  <!doctype html>\\n<html lang=\"en\" dir=\"ltr\" cla...             None   \n",
       "5  <!doctype html>\\n<html lang=\"en\" dir=\"ltr\" cla...             None   \n",
       "6  <!doctype html>\\n<html lang=\"en\" dir=\"ltr\" cla...             None   \n",
       "7  <!doctype html>\\n<html lang=\"en\" dir=\"ltr\" cla...             None   \n",
       "8  <!doctype html>\\n<html lang=\"en\" dir=\"ltr\" cla...             None   \n",
       "9  <!doctype html>\\n<html lang=\"en\" dir=\"ltr\" cla...             None   \n",
       "\n",
       "             status  \n",
       "0    scrapy_success  \n",
       "1  firecrawl_failed  \n",
       "2    scrapy_success  \n",
       "3    scrapy_success  \n",
       "4    scrapy_success  \n",
       "5    scrapy_success  \n",
       "6    scrapy_success  \n",
       "7    scrapy_success  \n",
       "8    scrapy_success  \n",
       "9    scrapy_success  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_ua():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "firecrawl = FirecrawlApp(api_key=firecrawl_api_key)\n",
    "\n",
    "def firecrawl_scrape(url):\n",
    "    try:\n",
    "        result = firecrawl.scrape_url(\n",
    "            url,\n",
    "            params={\n",
    "                \"formats\": [\"html\"],\n",
    "                \"timeout\": 15000\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if result and result.get(\"html\"):\n",
    "            return {\n",
    "                \"html\": result.get(\"html\"),\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Firecrawl exception for {url}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "results = []\n",
    "\n",
    "class MultiURLSpider(scrapy.Spider):\n",
    "    name = \"multi_url_spider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        for _, row in page_indexing.iterrows():\n",
    "            page_id = row[\"id\"]\n",
    "            page_url = row[\"page_url\"]\n",
    "\n",
    "            if pd.isna(page_url):\n",
    "                continue\n",
    "\n",
    "            print(f\"‚û°Ô∏è Requesting {page_id} | {page_url}\")\n",
    "\n",
    "            yield scrapy.Request(\n",
    "                url=page_url,\n",
    "                callback=self.parse,\n",
    "                errback=self.handle_error,\n",
    "                cb_kwargs={\"page_id\": page_id, \"page_url\": page_url},\n",
    "                headers={\"User-Agent\": get_random_ua()},\n",
    "                meta={\"download_timeout\": 15, \"max_retry_times\": 2},\n",
    "                dont_filter=True\n",
    "            )\n",
    "\n",
    "    def parse(self, response, page_id, page_url):\n",
    "        print(f\"‚úÖ Scrapy success: {page_url}\")\n",
    "        results.append({\n",
    "            \"page_id\": page_id,\n",
    "            \"page_url\": page_url,\n",
    "            \"html_content\": response.text,\n",
    "            \"status\": \"scrapy_success\"\n",
    "        })\n",
    "\n",
    "    def handle_error(self, failure):\n",
    "        request = failure.request\n",
    "        page_id = request.cb_kwargs[\"page_id\"]\n",
    "        page_url = request.cb_kwargs[\"page_url\"]\n",
    "\n",
    "        print(f\"‚ö†Ô∏è Scrapy failed: {page_url} ‚Üí trying requests\")\n",
    "\n",
    "        # ---- Fallback #1: requests ----\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                page_url,\n",
    "                headers={\"User-Agent\": get_random_ua()},\n",
    "                timeout=15\n",
    "            )\n",
    "\n",
    "            if r.status_code == 200 and r.text.strip():\n",
    "                print(f\"‚úÖ Requests fallback success: {page_url}\")\n",
    "                results.append({\n",
    "                    \"page_id\": page_id,\n",
    "                    \"page_url\": page_url,\n",
    "                    \"html_content\": r.text,\n",
    "                    \"status\": \"requests_fallback_success\"\n",
    "                })\n",
    "                return\n",
    "\n",
    "            print(f\"‚ö†Ô∏è Requests failed ({r.status_code})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Requests exception: {e}\")\n",
    "\n",
    "        # ---- Fallback #2: Firecrawl ----\n",
    "        print(f\"üî• Trying Firecrawl: {page_url}\")\n",
    "        firecrawl_result = firecrawl_scrape(page_url)\n",
    "\n",
    "        if firecrawl_result:\n",
    "            print(f\"‚úÖ Firecrawl success: {page_url}\")\n",
    "            results.append({\n",
    "                \"page_id\": page_id,\n",
    "                \"page_url\": page_url,\n",
    "                \"html_content\": firecrawl_result[\"html\"],\n",
    "                \"status\": \"firecrawl_success\"\n",
    "            })\n",
    "        else:\n",
    "            print(f\"‚ùå Firecrawl failed: {page_url}\")\n",
    "            results.append({\n",
    "                \"page_id\": page_id,\n",
    "                \"page_url\": page_url,\n",
    "                \"html_content\": None,\n",
    "                \"status\": \"firecrawl_failed\"\n",
    "            })\n",
    "\n",
    "# Run spider\n",
    "process = CrawlerProcess(settings={\"LOG_LEVEL\": \"ERROR\"})\n",
    "process.crawl(MultiURLSpider)\n",
    "process.start()\n",
    "\n",
    "# Results list\n",
    "page_scrape_list = pd.DataFrame(results)\n",
    "page_scrape_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea5275-e93d-488a-820d-8fee55f90fc0",
   "metadata": {},
   "source": [
    "## Convert to markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9287e6c9-86d3-4ad8-ba23-1cc33c71264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Readable text and tagline extraction complete\n"
     ]
    }
   ],
   "source": [
    "# Initialize html2text converter\n",
    "converter = html2text.HTML2Text()\n",
    "converter.ignore_links = False\n",
    "converter.ignore_images = True \n",
    "converter.ignore_tables = False  \n",
    "converter.ignore_emphasis = True\n",
    "converter.body_width = 0\n",
    "\n",
    "# Function to extract readable text and tagline from stored HTML content\n",
    "def extract_text_and_tagline(html_content):\n",
    "    if pd.isna(html_content) or not isinstance(html_content, str) or html_content.strip() == \"\":\n",
    "        return pd.Series([None, None], index=[\"readable_text_new\", \"tagline\"])\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        h1 = soup.find(\"h1\")\n",
    "        tagline = h1.get_text(strip=True) if h1 else None\n",
    " \n",
    "        body = soup.body\n",
    "        if not body:\n",
    "            return pd.Series([None, tagline], index=[\"readable_text_new\", \"tagline\"])\n",
    "\n",
    "        # Remove common unwanted elements inside body\n",
    "        for tag in body.find_all([\"script\", \"style\", \"nav\", \"footer\", \"aside\", \"form\", \"noscript\", \"header\"]):\n",
    "            tag.extract()\n",
    "\n",
    "        # Remove empty or standalone anchor links that cause `[](/)` issues\n",
    "        for tag in body.find_all(\"a\"):\n",
    "            if not tag.text.strip():\n",
    "                tag.extract()\n",
    "\n",
    "        # Convert to readable markdown-style text\n",
    "        extracted_text = converter.handle(str(body)).strip()\n",
    "        extracted_text = extracted_text.replace(\"[](/)\", \"\").strip()\n",
    "\n",
    "        return pd.Series([extracted_text, tagline], index=[\"readable_text_new\", \"tagline\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing HTML: {str(e)}\")\n",
    "        return pd.Series([f\"Error: {str(e)}\", None], index=[\"readable_text_new\", \"tagline\"])\n",
    "\n",
    "# Apply transformation to extract readable text and tagline\n",
    "page_scrape_list[[\"readable_text_new\", \"tagline\"]] = page_scrape_list[\"html_content\"].apply(extract_text_and_tagline)\n",
    "print(\"‚úÖ Readable text and tagline extraction complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c871f2e-c265-404e-9c68-a7ba9ded1809",
   "metadata": {},
   "source": [
    "## Compare and contrast versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a86c0676-181d-4153-bbde-0000a522f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing page changes\n",
      "page_status\n",
      "no_update_detected    9\n",
      "update_detected       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Ensure created_at is datetime\n",
    "change_log['created_at'] = pd.to_datetime(change_log['created_at'], format='mixed', utc=True)\n",
    "\n",
    "# --- Build a set of page_ids that have *any* historical record\n",
    "pages_with_history = set(change_log['page_id'].unique())\n",
    "\n",
    "# --- Merge the most recent row (even if readable_text is NaN)\n",
    "latest_per_page = (\n",
    "    change_log\n",
    "      .sort_values('created_at', ascending=False)\n",
    "      .drop_duplicates(subset='page_id', keep='first')\n",
    "      [['page_id', 'readable_text']]\n",
    "      .rename(columns={'readable_text': 'previous_readable_text'})\n",
    ")\n",
    "\n",
    "updated_change_log = page_scrape_list.merge(latest_per_page, on='page_id', how='left')\n",
    "\n",
    "# --- Status rules:\n",
    "# 1) page_id never in change_log  -> initial_scrape\n",
    "# 2) page_id exists and new text differs from previous -> update_detected\n",
    "# 3) else -> no_update_detected\n",
    "has_prev_record = updated_change_log['page_id'].isin(pages_with_history)\n",
    "has_new_text    = updated_change_log['readable_text_new'].notna()\n",
    "\n",
    "def _norm(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.fillna('')\n",
    "         .astype(str)\n",
    "         .str.replace(r'\\s+', ' ', regex=True)\n",
    "         .str.strip()\n",
    "    )\n",
    "\n",
    "prev_norm = _norm(updated_change_log['previous_readable_text'])\n",
    "new_norm  = _norm(updated_change_log['readable_text_new'])\n",
    "\n",
    "cond_initial = ~has_prev_record\n",
    "cond_update  = has_prev_record & has_new_text & (new_norm != prev_norm)\n",
    "\n",
    "updated_change_log['page_status'] = np.select(\n",
    "    [cond_initial, cond_update],\n",
    "    ['initial_scrape', 'update_detected'],\n",
    "    default='no_update_detected'\n",
    ")\n",
    "\n",
    "print(\"Comparing page changes\")\n",
    "print(updated_change_log['page_status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7611cf8-a822-4e12-a999-5c4938a96db6",
   "metadata": {},
   "source": [
    "## Drop rows where no change detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a5ab04f-41b8-4c4c-9acb-874b056bf1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes or initial scraped detected for 1 pages\n"
     ]
    }
   ],
   "source": [
    "filtered_df = updated_change_log[updated_change_log[\"page_status\"] != \"no_update_detected\"]\n",
    "updated_change_log = filtered_df\n",
    "print(f\"Changes or initial scraped detected for {len(updated_change_log)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e8e6ff-2729-4b64-81e6-635a7677284b",
   "metadata": {},
   "source": [
    "## Send updates to supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afffcd09-85e8-4dd7-86e9-854b476cec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inserted 1 records\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "records_to_insert = []\n",
    "\n",
    "for _, row in updated_change_log.iterrows():\n",
    "    page_id = int(row[\"page_id\"])\n",
    "    readable_text = row[\"readable_text_new\"]\n",
    "    page_status = row[\"page_status\"]\n",
    "    tagline = row[\"tagline\"]\n",
    "\n",
    "    # Skip row if \\u0000 is in readable_text or page_status\n",
    "    if isinstance(readable_text, str) and '\\u0000' in readable_text:\n",
    "        print(f\"‚ö†Ô∏è Skipping page_id {page_id} due to null byte in readable_text\")\n",
    "        continue\n",
    "    if isinstance(page_status, str) and '\\u0000' in page_status:\n",
    "        print(f\"‚ö†Ô∏è Skipping page_id {page_id} due to null byte in page_status\")\n",
    "        continue\n",
    "\n",
    "    records_to_insert.append({\n",
    "        \"page_id\": page_id,\n",
    "        \"readable_text\": readable_text,\n",
    "        \"page_status\": page_status,\n",
    "        \"h1_copy\": tagline,\n",
    "        \"processed\": False,\n",
    "    })\n",
    "\n",
    "    # When we hit the batch size, send them\n",
    "    if len(records_to_insert) >= batch_size:\n",
    "        response = supabase.table(\"change_log\").insert(records_to_insert).execute()\n",
    "        if response.data:\n",
    "            print(f\"‚úÖ Inserted {len(records_to_insert)} records\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to insert {len(records_to_insert)} records\")\n",
    "        records_to_insert = []\n",
    "\n",
    "# Insert any remaining records\n",
    "if records_to_insert:\n",
    "    response = supabase.table(\"change_log\").insert(records_to_insert).execute()\n",
    "    if response.data:\n",
    "        print(f\"‚úÖ Inserted {len(records_to_insert)} records\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to insert {len(records_to_insert)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b139848-d170-41d6-a5ee-2b22db8228c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
