{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013f22db-44e3-4d40-a7c0-80d511d605b0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8af9842-2723-403c-848b-878b2f2b989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Starting to pull news articles...\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úÖ Starting to pull news articles...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39be90fc-4135-4445-ad87-c74368791087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import requests, time\n",
    "from urllib.parse import urlparse\n",
    "from supabase import create_client, Client\n",
    "\n",
    "#Supbase\n",
    "SUPABASE_URL = os.environ[\"SUPABASE_URL\"]\n",
    "SUPABASE_KEY = os.environ[\"SUPABASE_KEY\"]\n",
    "SERVICE_ROLE_KEY = os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"]\n",
    "\n",
    "supabase: Client = create_client(SUPABASE_URL, SERVICE_ROLE_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694010f-f5da-4ec1-b396-fd405672e4bf",
   "metadata": {},
   "source": [
    "# Grab data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b46579-0076-4845-b04d-291a970c24e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully fetched `companies` table with filter '{'status': ['trial', 'active']}' and 6 rows.\n",
      "‚úÖ Successfully fetched `competitors` table with filter 'None' and 60 rows.\n",
      "‚úÖ Filtered to 6 trial and active accounts with 26 competitors\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Function to Fetch Data from Supabase\n",
    "def fetch_data(table_name, batch_size=500, filters=None, related_tables=None):\n",
    "    try:\n",
    "        all_data = []\n",
    "        start = 0\n",
    "\n",
    "        # Build select string\n",
    "        if related_tables:\n",
    "            select_string = \"*, \" + \", \".join(f\"{tbl}(*)\" for tbl in related_tables)\n",
    "        else:\n",
    "            select_string = \"*\"\n",
    "\n",
    "        while True:\n",
    "            query = supabase.table(table_name).select(select_string)\n",
    "            \n",
    "            if filters:\n",
    "                for column, value in filters.items():\n",
    "                    if isinstance(value, list):\n",
    "                        query = query.in_(column, value)\n",
    "                    elif value is None:\n",
    "                        query = query.is_(column, None)\n",
    "                    else:\n",
    "                        query = query.eq(column, value)\n",
    "            \n",
    "            response = query.range(start, start + batch_size - 1).execute()\n",
    "            \n",
    "            if response.data:\n",
    "                all_data.extend(response.data)\n",
    "                start += batch_size\n",
    "                if len(response.data) < batch_size:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if all_data:\n",
    "            print(f\"‚úÖ Successfully fetched `{table_name}` table with filter '{filters}' and {len(all_data)} rows.\")\n",
    "            return pd.DataFrame(all_data)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è `{table_name}` is empty.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching data from '{table_name}': {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ‚úÖ Fetch data from tables\n",
    "companies = fetch_data(\n",
    "    \"companies\",\n",
    "     filters={\"status\": [\"trial\", \"active\"]},\n",
    ")\n",
    "competitors = fetch_data(\n",
    "    \"competitors\",\n",
    ")\n",
    "\n",
    "# Filter competitors to only trial accounts\n",
    "filtered_competitors = competitors[competitors[\"company_id\"].isin(companies[\"id\"])]\n",
    "\n",
    "# Optional: reset index if you want a clean one\n",
    "filtered_competitors = filtered_competitors.reset_index(drop=True)\n",
    "competitors = filtered_competitors\n",
    "print(f\"‚úÖ Filtered to {len(companies)} trial and active accounts with {len(competitors)} competitors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701f213-2840-4025-ad34-ca73b8192c86",
   "metadata": {},
   "source": [
    "# Collect articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d664ef-4ac4-4233-9420-c12aef424898",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://google-news13.p.rapidapi.com/search\"\n",
    "headers = {\n",
    "    \"x-rapidapi-key\": \"e88f5d3d95msh96c8e7a091f4a90p1bee3cjsn2b537c4486ca\",\n",
    "    \"x-rapidapi-host\": \"google-news13.p.rapidapi.com\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51985a6d-d231-4a48-a834-c5561ec9fea9",
   "metadata": {},
   "source": [
    "## Collects news for competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8510050d-a710-4f1a-95b9-7bae0ddf0a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Searching for competitor hampr using:\n",
      "   1) Query: hampr\n",
      "   2) Domain: tryhampr.com\n",
      "   ‚Üí Found 19 articles for 'hampr'\n",
      "   ‚Üí Found 5 articles for 'tryhampr.com'\n",
      "\n",
      "üîé Searching for competitor Oracle Health using:\n",
      "   1) Query: Oracle Health\n",
      "   2) Domain: oracle.com\n",
      "   ‚Üí Found 100 articles for 'Oracle Health'\n",
      "   ‚Üí Found 100 articles for 'oracle.com'\n",
      "\n",
      "üîé Searching for competitor HappyNest using:\n",
      "   1) Query: HappyNest\n",
      "   2) Domain: happynest.com\n",
      "   ‚Üí Found 52 articles for 'HappyNest'\n",
      "   ‚Üí Found 32 articles for 'happynest.com'\n",
      "\n",
      "üîé Searching for competitor Pieces using:\n",
      "   1) Query: Pieces\n",
      "   2) Domain: piecestech.com\n",
      "   ‚Üí Found 100 articles for 'Pieces'\n",
      "   ‚Üí Found 6 articles for 'piecestech.com'\n",
      "\n",
      "üîé Searching for competitor Suki using:\n",
      "   1) Query: Suki\n",
      "   2) Domain: suki.ai\n",
      "   ‚Üí Found 100 articles for 'Suki'\n",
      "   ‚Üí Found 100 articles for 'suki.ai'\n",
      "\n",
      "üîé Searching for competitor Workiz using:\n",
      "   1) Query: Workiz\n",
      "   2) Domain: workiz.com\n",
      "   ‚Üí Found 29 articles for 'Workiz'\n",
      "   ‚Üí Found 18 articles for 'workiz.com'\n",
      "\n",
      "üîé Searching for competitor Simply Wise using:\n",
      "   1) Query: Simply Wise\n",
      "   2) Domain: simplywise.com\n",
      "   ‚Üí Found 63 articles for 'Simply Wise'\n",
      "   ‚Üí Found 3 articles for 'simplywise.com'\n",
      "\n",
      "üîé Searching for competitor Turner using:\n",
      "   1) Query: Turner\n",
      "   2) Domain: turnerconstruction.com\n",
      "   ‚Üí Found 100 articles for 'Turner'\n",
      "   ‚Üí Found 85 articles for 'turnerconstruction.com'\n",
      "\n",
      "üîé Searching for competitor Field Pulse using:\n",
      "   1) Query: Field Pulse\n",
      "   2) Domain: fieldpulse.com\n",
      "   ‚Üí Found 100 articles for 'Field Pulse'\n",
      "   ‚Üí Found 12 articles for 'fieldpulse.com'\n",
      "\n",
      "üîé Searching for competitor Jobber using:\n",
      "   1) Query: Jobber\n",
      "   2) Domain: getjobber.com\n",
      "   ‚Üí Found 100 articles for 'Jobber'\n",
      "   ‚Üí Found 8 articles for 'getjobber.com'\n",
      "\n",
      "üîé Searching for competitor The Rockefeller Foundation using:\n",
      "   1) Query: The Rockefeller Foundation\n",
      "   2) Domain: rockefellerfoundation.org\n",
      "   ‚Üí Found 100 articles for 'The Rockefeller Foundation'\n",
      "   ‚Üí Found 100 articles for 'rockefellerfoundation.org'\n",
      "\n",
      "üîé Searching for competitor Triodos using:\n",
      "   1) Query: Triodos\n",
      "   2) Domain: triodos.com\n",
      "   ‚Üí Found 100 articles for 'Triodos'\n",
      "   ‚Üí Found 55 articles for 'triodos.com'\n",
      "\n",
      "üîé Searching for competitor Ashoka using:\n",
      "   1) Query: Ashoka\n",
      "   2) Domain: ashoka.org\n",
      "   ‚Üí Found 100 articles for 'Ashoka'\n",
      "   ‚Üí Found 61 articles for 'ashoka.org'\n",
      "\n",
      "üîé Searching for competitor Best Mate using:\n",
      "   1) Query: Best Mate\n",
      "   2) Domain: gobestmate.com\n",
      "   ‚Üí Found 100 articles for 'Best Mate'\n",
      "   ‚Üí Found 0 articles for 'gobestmate.com'\n",
      "\n",
      "üîé Searching for competitor Service Titan using:\n",
      "   1) Query: Service Titan\n",
      "   2) Domain: servicetitan.com\n",
      "   ‚Üí Found 100 articles for 'Service Titan'\n",
      "   ‚Üí Found 63 articles for 'servicetitan.com'\n",
      "\n",
      "üîé Searching for competitor Housecall Pro using:\n",
      "   1) Query: Housecall Pro\n",
      "   2) Domain: housecallpro.com\n",
      "   ‚Üí Found 52 articles for 'Housecall Pro'\n",
      "   ‚Üí Found 23 articles for 'housecallpro.com'\n",
      "\n",
      "üîé Searching for competitor Wex FSM using:\n",
      "   1) Query: Wex FSM\n",
      "   2) Domain: wexinc.com\n",
      "   ‚Üí Found 4 articles for 'Wex FSM'\n",
      "   ‚Üí Found 35 articles for 'wexinc.com'\n",
      "\n",
      "üîé Searching for competitor Ambience Healthcare using:\n",
      "   1) Query: Ambience Healthcare\n",
      "   2) Domain: ambiencehealthcare.com\n",
      "   ‚Üí Found 100 articles for 'Ambience Healthcare'\n",
      "   ‚Üí Found 49 articles for 'ambiencehealthcare.com'\n",
      "\n",
      "üîé Searching for competitor Skanksa using:\n",
      "   1) Query: Skanksa\n",
      "   2) Domain: skanska.com\n",
      "   ‚Üí Found 20 articles for 'Skanksa'\n",
      "   ‚Üí Found 50 articles for 'skanska.com'\n",
      "\n",
      "üîé Searching for competitor Clayco using:\n",
      "   1) Query: Clayco\n",
      "   2) Domain: claycorp.com\n",
      "   ‚Üí Found 100 articles for 'Clayco'\n",
      "   ‚Üí Found 32 articles for 'claycorp.com'\n",
      "\n",
      "üîé Searching for competitor Epic using:\n",
      "   1) Query: Epic\n",
      "   2) Domain: epic.com\n",
      "   ‚Üí Found 97 articles for 'Epic'\n",
      "   ‚Üí Found 77 articles for 'epic.com'\n",
      "\n",
      "üîé Searching for competitor Microsoft Healthcare using:\n",
      "   1) Query: Microsoft Healthcare\n",
      "   2) Domain: microsoft.com\n",
      "   ‚Üí Found 100 articles for 'Microsoft Healthcare'\n",
      "   ‚Üí Found 100 articles for 'microsoft.com'\n",
      "\n",
      "üîé Searching for competitor Nabla using:\n",
      "   1) Query: Nabla\n",
      "   2) Domain: nabla.com\n",
      "   ‚Üí Found 100 articles for 'Nabla'\n",
      "   ‚Üí Found 38 articles for 'nabla.com'\n",
      "\n",
      "üîé Searching for competitor Square using:\n",
      "   1) Query: Square\n",
      "   2) Domain: squareup.com\n",
      "   ‚Üí Found 100 articles for 'Square'\n",
      "   ‚Üí Found 100 articles for 'squareup.com'\n",
      "\n",
      "üîé Searching for competitor SpotOn using:\n",
      "   1) Query: SpotOn\n",
      "   2) Domain: spoton.com\n",
      "   ‚Üí Found 100 articles for 'SpotOn'\n",
      "   ‚Üí Found 51 articles for 'spoton.com'\n",
      "\n",
      "üîé Searching for competitor DoorDash using:\n",
      "   1) Query: DoorDash\n",
      "   2) Domain: doordash.com\n",
      "   ‚Üí Found 100 articles for 'DoorDash'\n",
      "   ‚Üí Found 100 articles for 'doordash.com'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>competitor_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>search_term</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>thumbnail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>tryhampr.com</td>\n",
       "      <td>hampr</td>\n",
       "      <td>1755673200000</td>\n",
       "      <td>Hampr lands $500K investment from Louisiana Gr...</td>\n",
       "      <td>Hampr, the on-demand laundry service, received...</td>\n",
       "      <td>https://www.theadvocate.com/acadiana/news/busi...</td>\n",
       "      <td>The Advocate</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iI0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>tryhampr.com</td>\n",
       "      <td>hampr</td>\n",
       "      <td>1741075200000</td>\n",
       "      <td>Dallas-Based Hampr Turns a Household Chore Int...</td>\n",
       "      <td>Founder Laurel Hess was attending a T-ball gam...</td>\n",
       "      <td>https://dallasinnovates.com/dallas-based-hampr...</td>\n",
       "      <td>Dallas Innovates</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>tryhampr.com</td>\n",
       "      <td>hampr</td>\n",
       "      <td>1669104000000</td>\n",
       "      <td>I Made $40,000 Last Year Washing Laundry on th...</td>\n",
       "      <td>I can have anywhere from 18 to 25 orders on re...</td>\n",
       "      <td>https://www.businessinsider.com/made-40000-was...</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>tryhampr.com</td>\n",
       "      <td>hampr</td>\n",
       "      <td>1582531200000</td>\n",
       "      <td>Lafayette's Hampr is preparing to bring on-dem...</td>\n",
       "      <td>Hampr charges a yearly membership fee of $39 a...</td>\n",
       "      <td>https://www.theadvertiser.com/story/money/busi...</td>\n",
       "      <td>The Daily Advertiser | Lafayette, Louisiana</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>tryhampr.com</td>\n",
       "      <td>hampr</td>\n",
       "      <td>1579161600000</td>\n",
       "      <td>Hampr, A New Local Startup Providing On-demand...</td>\n",
       "      <td>hampr is an on-demand platform where users can...</td>\n",
       "      <td>https://developinglafayette.com/wp/hampr-a-new...</td>\n",
       "      <td>Developing Lafayette -</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iL0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   competitor_id        domain search_term      timestamp  \\\n",
       "0             24  tryhampr.com       hampr  1755673200000   \n",
       "1             24  tryhampr.com       hampr  1741075200000   \n",
       "2             24  tryhampr.com       hampr  1669104000000   \n",
       "3             24  tryhampr.com       hampr  1582531200000   \n",
       "4             24  tryhampr.com       hampr  1579161600000   \n",
       "\n",
       "                                               title  \\\n",
       "0  Hampr lands $500K investment from Louisiana Gr...   \n",
       "1  Dallas-Based Hampr Turns a Household Chore Int...   \n",
       "2  I Made $40,000 Last Year Washing Laundry on th...   \n",
       "3  Lafayette's Hampr is preparing to bring on-dem...   \n",
       "4  Hampr, A New Local Startup Providing On-demand...   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  Hampr, the on-demand laundry service, received...   \n",
       "1  Founder Laurel Hess was attending a T-ball gam...   \n",
       "2  I can have anywhere from 18 to 25 orders on re...   \n",
       "3  Hampr charges a yearly membership fee of $39 a...   \n",
       "4  hampr is an on-demand platform where users can...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.theadvocate.com/acadiana/news/busi...   \n",
       "1  https://dallasinnovates.com/dallas-based-hampr...   \n",
       "2  https://www.businessinsider.com/made-40000-was...   \n",
       "3  https://www.theadvertiser.com/story/money/busi...   \n",
       "4  https://developinglafayette.com/wp/hampr-a-new...   \n",
       "\n",
       "                                     publisher  \\\n",
       "0                                 The Advocate   \n",
       "1                             Dallas Innovates   \n",
       "2                             Business Insider   \n",
       "3  The Daily Advertiser | Lafayette, Louisiana   \n",
       "4                       Developing Lafayette -   \n",
       "\n",
       "                                           thumbnail  \n",
       "0  https://news.google.com/api/attachments/CC8iI0...  \n",
       "1  https://news.google.com/api/attachments/CC8iK0...  \n",
       "2  https://news.google.com/api/attachments/CC8iK0...  \n",
       "3  https://news.google.com/api/attachments/CC8iK0...  \n",
       "4  https://news.google.com/api/attachments/CC8iL0...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_rows = []\n",
    "\n",
    "for index, row in competitors.iterrows():\n",
    "    competitor_id = row[\"id\"]\n",
    "    website_url = row[\"website_url\"]\n",
    "    competitor_name = row[\"competitor_name\"]\n",
    "\n",
    "    # Extract domain\n",
    "    parsed = urlparse(website_url)\n",
    "    domain = (parsed.netloc or parsed.path).replace(\"www.\", \"\").strip(\"/\")\n",
    "\n",
    "    # LLM-generated search query\n",
    "    # query = row[\"news_search_query\"]\n",
    "\n",
    "    # Temporarily replacing search query with competitor name\n",
    "    query = row[\"competitor_name\"]\n",
    "\n",
    "    # We now search *both*:\n",
    "    search_terms = [\n",
    "        query,\n",
    "        domain\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüîé Searching for competitor {competitor_name} using:\")\n",
    "    print(\"   1) Query:\", query)\n",
    "    print(\"   2) Domain:\", domain)\n",
    "\n",
    "    for term in search_terms:\n",
    "        params = {\"keyword\": term, \"lr\": \"en-US\"}\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            data = response.json()\n",
    "            items = data.get(\"items\", [])\n",
    "\n",
    "            print(f\"   ‚Üí Found {len(items)} articles for '{term}'\")\n",
    "\n",
    "            # Store results\n",
    "            for article in items:\n",
    "                news_rows.append({\n",
    "                    \"competitor_id\": competitor_id,\n",
    "                    \"domain\": domain,\n",
    "                    \"search_term\": term,\n",
    "                    \"timestamp\": article.get(\"timestamp\"),\n",
    "                    \"title\": article.get(\"title\"),\n",
    "                    \"snippet\": article.get(\"snippet\"),\n",
    "                    \"url\": article.get(\"newsUrl\"),\n",
    "                    \"publisher\": article.get(\"publisher\"),\n",
    "                    \"thumbnail\": article.get(\"images\", {}).get(\"thumbnail\")\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error searching '{term}': {e}\")\n",
    "\n",
    "        time.sleep(0.7)\n",
    "\n",
    "# Build DataFrame + dedupe URLs\n",
    "news_df = pd.DataFrame(news_rows)\n",
    "news_df = news_df.drop_duplicates(subset=[\"url\"])\n",
    "\n",
    "news_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41e98d-5de7-4bf4-8516-d786c4e861e1",
   "metadata": {},
   "source": [
    "## Collect news for company related keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb3e4cea-0f8a-4832-9e7e-3c38f261fc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>company_name</th>\n",
       "      <th>logo</th>\n",
       "      <th>website_url</th>\n",
       "      <th>status</th>\n",
       "      <th>reddit</th>\n",
       "      <th>primary_color</th>\n",
       "      <th>company_custom_prompt</th>\n",
       "      <th>news_topics_search</th>\n",
       "      <th>company_custom_feed_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, created_at, company_name, logo, website_url, status, reddit, primary_color, company_custom_prompt, news_topics_search, company_custom_feed_prompt]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies_with_keywords = companies[companies[\"news_topics_search\"].notnull()]\n",
    "companies_with_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f4f90f-2224-41fc-9eef-89657a7c27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_news_rows = []\n",
    "\n",
    "# for _, row in companies_with_keywords.iterrows():\n",
    "#     company_id = row[\"id\"]\n",
    "#     company_name = row.get(\"company_name\") or row.get(\"name\")  # just in case naming differs\n",
    "#     clusters = row[\"news_topics_search\"] or []\n",
    "\n",
    "#     print(f\"\\nüîé Searching for company {company_name} ({company_id}):\")\n",
    "\n",
    "#     # clusters is expected to be:\n",
    "#     # [{\"label\": \"...\", \"keywords\": [\"...\", \"...\"]}, ...]\n",
    "#     for cluster in clusters:\n",
    "#         label = cluster.get(\"label\", \"Unlabeled\")\n",
    "#         keywords = cluster.get(\"keywords\", []) or []\n",
    "\n",
    "#         for term in keywords:\n",
    "#             params = {\"keyword\": term, \"lr\": \"en-US\"}  # IMPORTANT: term (not the whole list)\n",
    "\n",
    "#             try:\n",
    "#                 response = requests.get(url, headers=headers, params=params, timeout=30)\n",
    "#                 data = response.json()\n",
    "#                 items = data.get(\"items\", [])\n",
    "\n",
    "#                 print(f\"   ‚Üí [{label}] '{term}': {len(items)} articles\")\n",
    "\n",
    "#                 for article in items:\n",
    "#                     keyword_news_rows.append({\n",
    "#                         \"company_id\": company_id,\n",
    "#                         \"company_name\": company_name,\n",
    "#                         \"topic_label\": label,\n",
    "#                         \"search_term\": term,\n",
    "#                         \"timestamp\": article.get(\"timestamp\"),\n",
    "#                         \"title\": article.get(\"title\"),\n",
    "#                         \"snippet\": article.get(\"snippet\"),\n",
    "#                         \"url\": article.get(\"newsUrl\"),\n",
    "#                         \"publisher\": article.get(\"publisher\"),\n",
    "#                         \"thumbnail\": (article.get(\"images\") or {}).get(\"thumbnail\"),\n",
    "#                         \"source\": \"google_news_api\"\n",
    "#                     })\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"‚ùå Error searching [{label}] '{term}': {e}\")\n",
    "\n",
    "#             time.sleep(0.7)\n",
    "\n",
    "# # Build DataFrame + dedupe URLs\n",
    "# keyword_news_df = pd.DataFrame(keyword_news_rows)\n",
    "\n",
    "# if not keyword_news_df.empty:\n",
    "#     keyword_news_df = keyword_news_df.drop_duplicates(subset=[\"url\"])\n",
    "\n",
    "# keyword_news_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7359ea-ef2b-4abf-baf5-dc718b6a32c8",
   "metadata": {},
   "source": [
    "## Convert timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba41b858-c9fa-4093-b5be-99233d186160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>competitor_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>search_term</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>thumbnail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>tryhampr.com</td>\n",
       "      <td>hampr</td>\n",
       "      <td>2025-08-20T07:00:00+00:00</td>\n",
       "      <td>Hampr lands $500K investment from Louisiana Gr...</td>\n",
       "      <td>Hampr, the on-demand laundry service, received...</td>\n",
       "      <td>https://www.theadvocate.com/acadiana/news/busi...</td>\n",
       "      <td>The Advocate</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iI0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>tryhampr.com</td>\n",
       "      <td>hampr</td>\n",
       "      <td>2025-03-04T08:00:00+00:00</td>\n",
       "      <td>Dallas-Based Hampr Turns a Household Chore Int...</td>\n",
       "      <td>Founder Laurel Hess was attending a T-ball gam...</td>\n",
       "      <td>https://dallasinnovates.com/dallas-based-hampr...</td>\n",
       "      <td>Dallas Innovates</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>tryhampr.com</td>\n",
       "      <td>hampr</td>\n",
       "      <td>2022-11-22T08:00:00+00:00</td>\n",
       "      <td>I Made $40,000 Last Year Washing Laundry on th...</td>\n",
       "      <td>I can have anywhere from 18 to 25 orders on re...</td>\n",
       "      <td>https://www.businessinsider.com/made-40000-was...</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>tryhampr.com</td>\n",
       "      <td>hampr</td>\n",
       "      <td>2020-02-24T08:00:00+00:00</td>\n",
       "      <td>Lafayette's Hampr is preparing to bring on-dem...</td>\n",
       "      <td>Hampr charges a yearly membership fee of $39 a...</td>\n",
       "      <td>https://www.theadvertiser.com/story/money/busi...</td>\n",
       "      <td>The Daily Advertiser | Lafayette, Louisiana</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>tryhampr.com</td>\n",
       "      <td>hampr</td>\n",
       "      <td>2020-01-16T08:00:00+00:00</td>\n",
       "      <td>Hampr, A New Local Startup Providing On-demand...</td>\n",
       "      <td>hampr is an on-demand platform where users can...</td>\n",
       "      <td>https://developinglafayette.com/wp/hampr-a-new...</td>\n",
       "      <td>Developing Lafayette -</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iL0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>158</td>\n",
       "      <td>doordash.com</td>\n",
       "      <td>doordash.com</td>\n",
       "      <td>2020-03-21T07:00:00+00:00</td>\n",
       "      <td>DoorDash Launches #OpenForDelivery Campaign To...</td>\n",
       "      <td>Over the past few days, cities and states have...</td>\n",
       "      <td>https://about.doordash.com/en-us/news/doordash...</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>158</td>\n",
       "      <td>doordash.com</td>\n",
       "      <td>doordash.com</td>\n",
       "      <td>2025-05-16T07:00:00+00:00</td>\n",
       "      <td>DoorDash partners with Velocity Frequent Flyer</td>\n",
       "      <td>By linking a Velocity account to DoorDash, mem...</td>\n",
       "      <td>https://retailworldmagazine.com.au/doordash-pa...</td>\n",
       "      <td>Retail World Magazine</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>158</td>\n",
       "      <td>doordash.com</td>\n",
       "      <td>doordash.com</td>\n",
       "      <td>2025-01-15T08:00:00+00:00</td>\n",
       "      <td>Georgia DoorDash drivers could be eligible soo...</td>\n",
       "      <td>DoorDash drivers are possibly eligible for new...</td>\n",
       "      <td>https://www.augustachronicle.com/story/news/20...</td>\n",
       "      <td>The Augusta Chronicle</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>158</td>\n",
       "      <td>doordash.com</td>\n",
       "      <td>doordash.com</td>\n",
       "      <td>2017-06-22T07:00:00+00:00</td>\n",
       "      <td>Four years in and just getting started</td>\n",
       "      <td>Four years in and just getting started. Four y...</td>\n",
       "      <td>https://about.doordash.com/en-us/news/four-yea...</td>\n",
       "      <td>DoorDash</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3438</th>\n",
       "      <td>158</td>\n",
       "      <td>doordash.com</td>\n",
       "      <td>doordash.com</td>\n",
       "      <td>2023-06-26T07:00:00+00:00</td>\n",
       "      <td>DOUBLE DEAL-IVERY ALERT: WENDY'S AND DOORDASH ...</td>\n",
       "      <td>From a.m. to p.m., Wendy's and DoorDash deligh...</td>\n",
       "      <td>https://www.irwendys.com/news/news-details/202...</td>\n",
       "      <td>The Wendy's Company - Investor Relations</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iMk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3143 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      competitor_id        domain   search_term                  timestamp  \\\n",
       "0                24  tryhampr.com         hampr  2025-08-20T07:00:00+00:00   \n",
       "1                24  tryhampr.com         hampr  2025-03-04T08:00:00+00:00   \n",
       "2                24  tryhampr.com         hampr  2022-11-22T08:00:00+00:00   \n",
       "3                24  tryhampr.com         hampr  2020-02-24T08:00:00+00:00   \n",
       "4                24  tryhampr.com         hampr  2020-01-16T08:00:00+00:00   \n",
       "...             ...           ...           ...                        ...   \n",
       "3434            158  doordash.com  doordash.com  2020-03-21T07:00:00+00:00   \n",
       "3435            158  doordash.com  doordash.com  2025-05-16T07:00:00+00:00   \n",
       "3436            158  doordash.com  doordash.com  2025-01-15T08:00:00+00:00   \n",
       "3437            158  doordash.com  doordash.com  2017-06-22T07:00:00+00:00   \n",
       "3438            158  doordash.com  doordash.com  2023-06-26T07:00:00+00:00   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Hampr lands $500K investment from Louisiana Gr...   \n",
       "1     Dallas-Based Hampr Turns a Household Chore Int...   \n",
       "2     I Made $40,000 Last Year Washing Laundry on th...   \n",
       "3     Lafayette's Hampr is preparing to bring on-dem...   \n",
       "4     Hampr, A New Local Startup Providing On-demand...   \n",
       "...                                                 ...   \n",
       "3434  DoorDash Launches #OpenForDelivery Campaign To...   \n",
       "3435     DoorDash partners with Velocity Frequent Flyer   \n",
       "3436  Georgia DoorDash drivers could be eligible soo...   \n",
       "3437             Four years in and just getting started   \n",
       "3438  DOUBLE DEAL-IVERY ALERT: WENDY'S AND DOORDASH ...   \n",
       "\n",
       "                                                snippet  \\\n",
       "0     Hampr, the on-demand laundry service, received...   \n",
       "1     Founder Laurel Hess was attending a T-ball gam...   \n",
       "2     I can have anywhere from 18 to 25 orders on re...   \n",
       "3     Hampr charges a yearly membership fee of $39 a...   \n",
       "4     hampr is an on-demand platform where users can...   \n",
       "...                                                 ...   \n",
       "3434  Over the past few days, cities and states have...   \n",
       "3435  By linking a Velocity account to DoorDash, mem...   \n",
       "3436  DoorDash drivers are possibly eligible for new...   \n",
       "3437  Four years in and just getting started. Four y...   \n",
       "3438  From a.m. to p.m., Wendy's and DoorDash deligh...   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://www.theadvocate.com/acadiana/news/busi...   \n",
       "1     https://dallasinnovates.com/dallas-based-hampr...   \n",
       "2     https://www.businessinsider.com/made-40000-was...   \n",
       "3     https://www.theadvertiser.com/story/money/busi...   \n",
       "4     https://developinglafayette.com/wp/hampr-a-new...   \n",
       "...                                                 ...   \n",
       "3434  https://about.doordash.com/en-us/news/doordash...   \n",
       "3435  https://retailworldmagazine.com.au/doordash-pa...   \n",
       "3436  https://www.augustachronicle.com/story/news/20...   \n",
       "3437  https://about.doordash.com/en-us/news/four-yea...   \n",
       "3438  https://www.irwendys.com/news/news-details/202...   \n",
       "\n",
       "                                        publisher  \\\n",
       "0                                    The Advocate   \n",
       "1                                Dallas Innovates   \n",
       "2                                Business Insider   \n",
       "3     The Daily Advertiser | Lafayette, Louisiana   \n",
       "4                          Developing Lafayette -   \n",
       "...                                           ...   \n",
       "3434                                     DoorDash   \n",
       "3435                        Retail World Magazine   \n",
       "3436                        The Augusta Chronicle   \n",
       "3437                                     DoorDash   \n",
       "3438     The Wendy's Company - Investor Relations   \n",
       "\n",
       "                                              thumbnail  \n",
       "0     https://news.google.com/api/attachments/CC8iI0...  \n",
       "1     https://news.google.com/api/attachments/CC8iK0...  \n",
       "2     https://news.google.com/api/attachments/CC8iK0...  \n",
       "3     https://news.google.com/api/attachments/CC8iK0...  \n",
       "4     https://news.google.com/api/attachments/CC8iL0...  \n",
       "...                                                 ...  \n",
       "3434                                               None  \n",
       "3435  https://news.google.com/api/attachments/CC8iK0...  \n",
       "3436  https://news.google.com/api/attachments/CC8iK0...  \n",
       "3437                                               None  \n",
       "3438  https://news.google.com/api/attachments/CC8iMk...  \n",
       "\n",
       "[3143 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df[\"timestamp\"] = news_df[\"timestamp\"].apply(\n",
    "    lambda ts: datetime.datetime.fromtimestamp(int(ts)/1000, tz=datetime.timezone.utc)\n",
    "        .strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    ")\n",
    "news_df\n",
    "\n",
    "# keyword_news_df[\"timestamp\"] = keyword_news_df[\"timestamp\"].apply(\n",
    "#     lambda ts: datetime.datetime.fromtimestamp(int(ts)/1000, tz=datetime.timezone.utc)\n",
    "#         .strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "# )\n",
    "# keyword_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0627946-6295-407a-8686-acdd695da94d",
   "metadata": {},
   "source": [
    "## Drop all rows old than two weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e4c175-6b65-45c6-b5ed-ab6b80c914e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 3143 rows\n",
      "After:  621 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>competitor_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>search_term</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>thumbnail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>152</td>\n",
       "      <td>oracle.com</td>\n",
       "      <td>Oracle Health</td>\n",
       "      <td>2026-02-02 17:51:42+00:00</td>\n",
       "      <td>Oracle Health Adds Order Creation Capabilities...</td>\n",
       "      <td>Expanded AI capabilities leverage ambient list...</td>\n",
       "      <td>https://www.oracle.com/news/announcement/oracl...</td>\n",
       "      <td>Oracle</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>152</td>\n",
       "      <td>oracle.com</td>\n",
       "      <td>Oracle Health</td>\n",
       "      <td>2026-02-05 08:36:40+00:00</td>\n",
       "      <td>Accenture Federal Services Selected to Support...</td>\n",
       "      <td>Accenture Federal Services, a subsidiary of Ac...</td>\n",
       "      <td>https://newsroom.accenture.com/news/2026/accen...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>152</td>\n",
       "      <td>oracle.com</td>\n",
       "      <td>Oracle Health</td>\n",
       "      <td>2026-02-06 13:53:31+00:00</td>\n",
       "      <td>\"Oracle considering drastic measures to financ...</td>\n",
       "      <td>Oracle's investments in AI may prove costly. I...</td>\n",
       "      <td>https://www.techzine.eu/news/infrastructure/13...</td>\n",
       "      <td>Techzine Global</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>152</td>\n",
       "      <td>oracle.com</td>\n",
       "      <td>Oracle Health</td>\n",
       "      <td>2026-02-03 09:53:10+00:00</td>\n",
       "      <td>Oracle Health Clinical AI Agent Adds Automated...</td>\n",
       "      <td>Oracle Health has expanded its Clinical AI Age...</td>\n",
       "      <td>https://hlth.com/insights/news/oracle-health-c...</td>\n",
       "      <td>HLTH</td>\n",
       "      <td>https://news.google.com/api/attachments/CC8iK0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>152</td>\n",
       "      <td>oracle.com</td>\n",
       "      <td>Oracle Health</td>\n",
       "      <td>2026-02-04 13:05:00+00:00</td>\n",
       "      <td>Multiple Canadian Healthcare Organizations Sel...</td>\n",
       "      <td>Lumeo Regional Health Information System in So...</td>\n",
       "      <td>https://www.nasdaq.com/press-release/multiple-...</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    competitor_id      domain    search_term                 timestamp  \\\n",
       "24            152  oracle.com  Oracle Health 2026-02-02 17:51:42+00:00   \n",
       "25            152  oracle.com  Oracle Health 2026-02-05 08:36:40+00:00   \n",
       "26            152  oracle.com  Oracle Health 2026-02-06 13:53:31+00:00   \n",
       "27            152  oracle.com  Oracle Health 2026-02-03 09:53:10+00:00   \n",
       "28            152  oracle.com  Oracle Health 2026-02-04 13:05:00+00:00   \n",
       "\n",
       "                                                title  \\\n",
       "24  Oracle Health Adds Order Creation Capabilities...   \n",
       "25  Accenture Federal Services Selected to Support...   \n",
       "26  \"Oracle considering drastic measures to financ...   \n",
       "27  Oracle Health Clinical AI Agent Adds Automated...   \n",
       "28  Multiple Canadian Healthcare Organizations Sel...   \n",
       "\n",
       "                                              snippet  \\\n",
       "24  Expanded AI capabilities leverage ambient list...   \n",
       "25  Accenture Federal Services, a subsidiary of Ac...   \n",
       "26  Oracle's investments in AI may prove costly. I...   \n",
       "27  Oracle Health has expanded its Clinical AI Age...   \n",
       "28  Lumeo Regional Health Information System in So...   \n",
       "\n",
       "                                                  url        publisher  \\\n",
       "24  https://www.oracle.com/news/announcement/oracl...           Oracle   \n",
       "25  https://newsroom.accenture.com/news/2026/accen...        Accenture   \n",
       "26  https://www.techzine.eu/news/infrastructure/13...  Techzine Global   \n",
       "27  https://hlth.com/insights/news/oracle-health-c...             HLTH   \n",
       "28  https://www.nasdaq.com/press-release/multiple-...           Nasdaq   \n",
       "\n",
       "                                            thumbnail  \n",
       "24  https://news.google.com/api/attachments/CC8iK0...  \n",
       "25  https://news.google.com/api/attachments/CC8iK0...  \n",
       "26  https://news.google.com/api/attachments/CC8iK0...  \n",
       "27  https://news.google.com/api/attachments/CC8iK0...  \n",
       "28                                               None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df[\"timestamp\"] = pd.to_datetime(\n",
    "    news_df[\"timestamp\"],\n",
    "    utc=True,\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Define cutoff 1 week ago\n",
    "cutoff = pd.Timestamp.utcnow() - pd.Timedelta(weeks=1)\n",
    "\n",
    "# Filter\n",
    "filtered_df = news_df[news_df[\"timestamp\"] >= cutoff].copy()\n",
    "\n",
    "print(f\"Before: {len(news_df)} rows\")\n",
    "print(f\"After:  {len(filtered_df)} rows\")\n",
    "news_df = filtered_df\n",
    "news_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68358634-ecf6-4412-987f-daa223189f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure proper datetime format\n",
    "# keyword_news_df[\"timestamp\"] = pd.to_datetime(\n",
    "#     keyword_news_df[\"timestamp\"],\n",
    "#     # unit=\"ms\",\n",
    "#     utc=True\n",
    "# )\n",
    "\n",
    "# # Define cutoff 1 week ago\n",
    "# cutoff = pd.Timestamp.utcnow() - pd.Timedelta(weeks=1)\n",
    "\n",
    "# # Filter\n",
    "# filtered_df = keyword_news_df[keyword_news_df[\"timestamp\"] >= cutoff].copy()\n",
    "\n",
    "# print(f\"Before: {len(keyword_news_df)} rows\")\n",
    "# print(f\"After:  {len(filtered_df)} rows\")\n",
    "# keyword_news_df = filtered_df\n",
    "# keyword_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141eb13a-274c-4e44-afbe-49d832ae28f8",
   "metadata": {},
   "source": [
    "# Send to supabase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42298a8c-49f4-449d-9097-53048ee8475d",
   "metadata": {},
   "source": [
    "## Competitor related news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd8f802-60d2-40b7-97fd-57b74a5feb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert complete ‚Äî 621 rows inserted/updated\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for _, row in news_df.iterrows():\n",
    "    rows.append({\n",
    "        \"competitor_id\": row[\"competitor_id\"],\n",
    "        \"published_date\": row[\"timestamp\"].isoformat() if pd.notna(row[\"timestamp\"]) else None,\n",
    "        \"publisher\": row[\"publisher\"],\n",
    "        \"title\": row[\"title\"],\n",
    "        \"description\": row[\"snippet\"],\n",
    "        \"url\": row[\"url\"],\n",
    "        \"thumbnail\": row[\"thumbnail\"]\n",
    "    })\n",
    "\n",
    "response = supabase.table(\"news_feed\").upsert(\n",
    "    rows,\n",
    "    on_conflict=\"url\"\n",
    ").execute()\n",
    "\n",
    "count = len(response.data) if response.data else 0\n",
    "\n",
    "print(f\"Upsert complete ‚Äî {count} rows inserted/updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b85309-69f9-4487-ad2d-43a132b6c9da",
   "metadata": {},
   "source": [
    "## Company keyword related news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "843b3d4a-95b7-4d31-9780-2f4e64c4d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_news_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec4da862-6c97-4bed-bd7b-2841737e9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = []\n",
    "\n",
    "# for _, row in keyword_news_df.iterrows():\n",
    "#     rows.append({\n",
    "#         \"company_id\": row[\"company_id\"],\n",
    "#         \"published_date\": row[\"timestamp\"].isoformat() if pd.notna(row[\"timestamp\"]) else None,\n",
    "#         \"publisher\": row[\"publisher\"],\n",
    "#         \"title\": row[\"title\"],\n",
    "#         \"description\": row[\"snippet\"],\n",
    "#         \"url\": row[\"url\"],\n",
    "#         \"thumbnail\": row[\"thumbnail\"]\n",
    "#     })\n",
    "\n",
    "# response = supabase.table(\"news_feed\").upsert(\n",
    "#     rows,\n",
    "#     on_conflict=\"url\"\n",
    "# ).execute()\n",
    "\n",
    "# count = len(response.data) if response.data else 0\n",
    "\n",
    "# print(f\"Upsert complete ‚Äî {count} rows inserted/updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd7c34-9df7-4365-8ca9-00e72eb9feab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
